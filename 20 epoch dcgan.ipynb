{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOSvIBmMEuLFu9tnywCM61h"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1GI5aEBUAk0fqBBOdAZnr-o3Y8FclNkHz"},"id":"2kW5yGAXhWKf","executionInfo":{"status":"ok","timestamp":1763499311874,"user_tz":-630,"elapsed":37346563,"user":{"displayName":"Akhila R Nair","userId":"01448812229620485935"}},"outputId":"7ff48f2a-52d7-432f-d6e9-bb9ea027e439"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Hyperparameters\n","image_size = 32\n","batch_size = 128\n","latent_dim = 100\n","num_epochs = 20\n","learning_rate = 0.0002\n","beta1 = 0.5\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Data loading and preprocessing\n","transform = transforms.Compose([\n","    transforms.Resize(image_size),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*3, [0.5]*3)  # normalize to [-1,1]\n","])\n","\n","dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# Generator model\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            # input is latent vector Z\n","            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","\n","            nn.ConvTranspose2d(128, 3, 4, 2, 1, bias=False),\n","            nn.Tanh()  # output in range [-1,1]\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","# Discriminator model\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Conv2d(3, 128, 4, 2, 1, bias=False),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n","            nn.BatchNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace=True),\n","\n","            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n","            nn.Sigmoid()  # output probability\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x).view(-1)\n","\n","# Initialize models\n","G = Generator().to(device)\n","D = Discriminator().to(device)\n","\n","# Loss and optimizers\n","criterion = nn.BCELoss()\n","optimizerD = optim.Adam(D.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n","optimizerG = optim.Adam(G.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n","\n","# Fixed noise for monitoring progress\n","fixed_noise = torch.randn(64, latent_dim, 1, 1, device=device)\n","\n","def denorm(x):\n","    return x * 0.5 + 0.5\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    for i, (images, _) in enumerate(dataloader):\n","        real_images = images.to(device)\n","        batch_size_curr = real_images.size(0)\n","\n","        # Real and fake labels\n","        real_labels = torch.ones(batch_size_curr, device=device)\n","        fake_labels = torch.zeros(batch_size_curr, device=device)\n","\n","        # Train discriminator with real images\n","        outputs = D(real_images)\n","        d_loss_real = criterion(outputs, real_labels)\n","        D.zero_grad()\n","        d_loss_real.backward()\n","\n","        # Train discriminator with fake images\n","        noise = torch.randn(batch_size_curr, latent_dim, 1, 1, device=device)\n","        fake_images = G(noise)\n","        outputs = D(fake_images.detach())\n","        d_loss_fake = criterion(outputs, fake_labels)\n","        d_loss_fake.backward()\n","        optimizerD.step()\n","\n","        # Train generator\n","        G.zero_grad()\n","        outputs = D(fake_images)\n","        g_loss = criterion(outputs, real_labels)  # wants discriminator to mistake fakes for real\n","        g_loss.backward()\n","        optimizerG.step()\n","\n","        if (i+1) % 100 == 0:\n","            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], '\n","                  f'D_loss: {(d_loss_real + d_loss_fake).item():.4f}, G_loss: {g_loss.item():.4f}')\n","\n","    # Save images for monitoring\n","    with torch.no_grad():\n","        fake_images = G(fixed_noise).detach().cpu()\n","    plt.figure(figsize=(8,8))\n","    plt.axis('off')\n","    plt.title(f'Generated Images at Epoch {epoch+1}')\n","    plt.imshow(np.transpose(denorm(fake_images), (0,2,3,1)).reshape(8,8,image_size,image_size,3).swapaxes(1,2).reshape(8*image_size,8*image_size,3))\n","    plt.show()\n"]}]}